{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "train = datasets.MNIST(\"\", train=True, download=True,\n",
    "                      transform = transforms.Compose([transforms.ToTensor()]))\n",
    "test = datasets.MNIST(\"\", train=False, download=True,\n",
    "                      transform = transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "train_set = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)\n",
    "test_set = torch.utils.data.DataLoader(test, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#more like object orientated programming\n",
    "import torch.nn as nn\n",
    "#Specific Functions - interchangable with tourch.nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "super corresponds to nn module, running the initilization for nn.Module. When you inherit, you inherit methods and attributes but the initialisation is not run if you want initialisation method to run of your parent class that you are inheriting from you have to run super init\n",
    "fc1 = 1st fully connected layer\n",
    "nnLinear(input, output) -> \n",
    "nnLinear - fully connected\n",
    "   if convolutional neural networks - nnconv\n",
    "we cant pass image -> flatted image with view. Pixels will be in rows\n",
    "input is flattened images -> 28*28\n",
    "making 3 layers of 64 neurons for hidden layers\n",
    "therefore output = 6 or whatever we want\n",
    "output- 10 classes - output layer only has 10 neurons\n",
    "\n",
    "definining path for data to take through these layers\n",
    "feed-forward neural network - data passes in one direction from one side to another\n",
    "\n",
    "data won't be scaled properly without activation function\n",
    "F.relu - recified linear -> running activation function over entire layer\n",
    "    whether or not the neruon is firing like human brain\n",
    "    most activation function is sigmoid (ranges from 0 - 1)\n",
    "    it keeps the outputs of these layers from exploding\n",
    "    can still occur when calculating losses / gradients\n",
    "    activation function runs on the output side of the neuron\n",
    "    output layer - we only want one of the neruons to fully fire\n",
    "        - want a probability distribution on the output\n",
    "        - F log solve max for multi class classification problem\n",
    "            - dimention which we want to apply softmax\n",
    "            - output layer will be a batch of tensor distributions\n",
    "        - dim = 0  distribution across the batches\n",
    "        - dim = 1 - distribution across actul output layer\n",
    "        \n",
    " you can through if logics in forward method to do what ever you want and come up with advanced models, gradients are automatically generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.softmax(x,dim=1)\n",
    "    \n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Inserting data into our model\n",
    "    -1 specifices these input will be unknown shape i.e. any size (be prepared for any size data)\n",
    "    \n",
    "    grad_fn -> LogSoftmaxBackward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand((28,28))\n",
    "X = X.view(-1,28*28)\n",
    "output = net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1034, 0.0883, 0.0903, 0.1098, 0.0980, 0.1074, 0.0900, 0.0927, 0.1138,\n",
       "         0.1062]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
