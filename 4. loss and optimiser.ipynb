{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "train = datasets.MNIST(\"\", train=True, download=True,\n",
    "                      transform = transforms.Compose([transforms.ToTensor()]))\n",
    "test = datasets.MNIST(\"\", train=False, download=True,\n",
    "                      transform = transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "train_set = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)\n",
    "test_set = torch.utils.data.DataLoader(test, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#more like object orientated programming\n",
    "import torch.nn as nn\n",
    "#Specific Functions - interchangable with tourch.nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "super corresponds to nn module, running the initilization for nn.Module. When you inherit, you inherit methods and attributes but the initialisation is not run if you want initialisation method to run of your parent class that you are inheriting from you have to run super init\n",
    "fc1 = 1st fully connected layer\n",
    "nnLinear(input, output) -> \n",
    "nnLinear - fully connected\n",
    "   if convolutional neural networks - nnconv\n",
    "we cant pass image -> flatted image with view. Pixels will be in rows\n",
    "input is flattened images -> 28*28\n",
    "making 3 layers of 64 neurons for hidden layers\n",
    "therefore output = 6 or whatever we want\n",
    "output- 10 classes - output layer only has 10 neurons\n",
    "\n",
    "definining path for data to take through these layers\n",
    "feed-forward neural network - data passes in one direction from one side to another\n",
    "\n",
    "data won't be scaled properly without activation function\n",
    "F.relu - recified linear -> running activation function over entire layer\n",
    "    whether or not the neruon is firing like human brain\n",
    "    most activation function is sigmoid (ranges from 0 - 1)\n",
    "    it keeps the outputs of these layers from exploding\n",
    "    can still occur when calculating losses / gradients\n",
    "    activation function runs on the output side of the neuron\n",
    "    output layer - we only want one of the neruons to fully fire\n",
    "        - want a probability distribution on the output\n",
    "        - F log solve max for multi class classification problem\n",
    "            - dimention which we want to apply softmax\n",
    "            - output layer will be a batch of tensor distributions\n",
    "        - dim = 0  distribution across the batches\n",
    "        - dim = 1 - distribution across actul output layer\n",
    "        \n",
    " you can through if logics in forward method to do what ever you want and come up with advanced models, gradients are automatically generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.softmax(x,dim=1)\n",
    "    \n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Inserting data into our model\n",
    "    -1 specifices these input will be unknown shape i.e. any size (be prepared for any size data)\n",
    "    \n",
    "    grad_fn -> LogSoftmaxBackward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([8, 0, 7, 0, 8, 6, 1, 0, 4, 2])]\n"
     ]
    }
   ],
   "source": [
    "#showing the data. firstly shown the actual data 28 x 28 and then the result in a batch of 10\n",
    "for data in train_set:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand((28,28))\n",
    "X = X.view(-1,28*28)\n",
    "output = net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1092, 0.1036, 0.0915, 0.0923, 0.0880, 0.0951, 0.1088, 0.1048, 0.1038,\n",
       "         0.1027]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss - Measure of how wrong is the model. \n",
    "Goal is to decrease loss.\n",
    "model isnt 100% confidnet - not totally correct. we want it to be 100% correct\n",
    "\n",
    "Optimiser - go through and adjust the weights based on the loss, gradients\n",
    "    - adjust all the possible weights that it can adjust such a way to lower the cost \n",
    "    - learning rate\n",
    "    - imagery of learning rate\n",
    "net.parameter - everything that is adjustable\n",
    "learning rate - lr\n",
    "    - dictates the size of the step that your optimizer will take to get to the minimum loss\n",
    "    - not caluclating the perfect cost fucntion where loss = 0 on each batch of data because of over fitting\n",
    "    - learning rate to tell optimizer - optimize to lower the loss but only take certain size steps, and then over time as you take those steps , the parameters might get overwritten, and what remains batch after batch will be the actual general principles.\n",
    "    - if learning rate is too big, it's never going to get to the optimal opint\n",
    "    - if learning rate is too small, it will get stuck\n",
    "    - choosing the right learning step - no method to find it\n",
    "    -decaying learning rate - learning rate gets smaller and smaller\n",
    "    \n",
    "To do:\n",
    "Calculate loss based on the output - model output vs targeted / desired output and calculate how wrong is this and apply this back to the entire network to adjust the weights based on the loss and keep passing data and optimise loss\n",
    "\n",
    "EPOCHS - iterations overall all of our data and pass it through the model, but also want to iterate at least a few time through our data. a full pass through of the data.\n",
    "Whole passes through our entire dataset\n",
    "\n",
    "Batches - law of diminishing returns - helps to generalise\n",
    "gradiant - starts at zero everytime or it will get added together every batch\n",
    "    - gradiant contains your loss - how wrong are you\n",
    "    - optimiser goes through these gradiants to optimize these weights i.e. net zero grab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.8231, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1., grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for data in train_set:\n",
    "        #data is a batch of feature sets and labels i.e. target for classification\n",
    "        X, y = data\n",
    "        #print(X[0]) # rows of pixel\n",
    "        #print(y[0]) # labeled / target result\n",
    "        #before you pass your data through your nerual network, you want to zero gradiant\n",
    "        net.zero_grad()\n",
    "        # calculated model output\n",
    "        output = net(X.view(-1, 28*28))\n",
    "        #now calculate loss\n",
    "        # two ways of calculating loss\n",
    "        #one hot vector - optimal model output is 1 value within the output vector is 1 and others are 0\n",
    "        # usually if labelled data is a one hot vector -> use mean square error\n",
    "        # labeled data y is not a vector -> cannot use mean square error\n",
    "        loss = F.nll_loss(output,y)\n",
    "        #back propagate the loss\n",
    "        loss.backward()\n",
    "        #adjust the weights for us\n",
    "        optimizer.step()\n",
    "    print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.948\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "#we dont want to count gradiants here - we just want to know how good is the network at this point:\n",
    "with torch.no_grad():\n",
    "    for data in test_set:\n",
    "        X,y = data\n",
    "        output = net(X.view(-1, 28*28))\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "print(\"Accuracy: \", round(correct/total, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8, grad_fn=<NotImplemented>)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "index_batch = 7\n",
    "plt.imshow(X[index_batch].view(28,28))\n",
    "plt.show()\n",
    "print(torch.argmax(net(X[index_batch].view(-1,28*28))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
